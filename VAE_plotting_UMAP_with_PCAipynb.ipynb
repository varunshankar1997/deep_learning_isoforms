{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a66c1950c502>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mumap_\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.models import Model, load_model\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import matplotlib\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import umap.umap_ as umap\n",
    "import tensorflow\n",
    "import umap.plot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "import datashader.bundling as bd\n",
    "import matplotlib.pyplot as plt\n",
    "import colorcet\n",
    "import matplotlib.colors\n",
    "import matplotlib.cm\n",
    "import bokeh.plotting as bpl\n",
    "import bokeh.transform as btr\n",
    "import holoviews as hv\n",
    "import holoviews.operation.datashader as hd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load paths to datasets \n",
    "Path_Isoforms = '/Users/stef_cm/Desktop/DL_Final/subset.tsv' \n",
    "Path = '/Users/stef_cm/Desktop/DL_Final/gtex_gene_expression.tsv' \n",
    "latent_dim = 16\n",
    "intermediate_dim = 32\n",
    "epochz= 2\n",
    "batch_size = 420\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the tools we need for our VAE , Plotting and data loading \n",
    "def load_data(DATA_PATH):\n",
    "    \"\"\"Loads the data and preprocesses it.\"\"\"\n",
    "    row_names = []\n",
    "    array = []\n",
    "    with open(DATA_PATH, 'r', encoding='utf-8') as infile:\n",
    "        next(infile)\n",
    "        for line in infile:\n",
    "            line = line.split(\"\\t\")\n",
    "            row_names.append(line[0])\n",
    "            array.append(line[1:])\n",
    "\n",
    "    expr = np.asarray(array, dtype=np.float32)\n",
    "    expr = np.log2(1+expr[:])\n",
    "    expr = expr / np.max(expr, axis=1, keepdims=True)\n",
    "    expr = np.nan_to_num(expr)\n",
    "    return expr, row_names\n",
    "\n",
    "\n",
    "def train(autoencoder, data , epochs=epochz):\n",
    "    opt = torch.optim.Adam(autoencoder.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        for x in data:\n",
    "            opt.zero_grad()\n",
    "            x_hat = autoencoder(x)\n",
    "            loss = ((x - x_hat)**2).sum()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    return autoencoder\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data and use the *load data * function \n",
    "\n",
    "x, row_names = load_data(Path)\n",
    "original_dim = x.shape[1]\n",
    "original_dim\n",
    "\n",
    "# Transform the inputs to tensors for our VAE and define optimizer for the model\n",
    "inputs = keras.Input(shape=(original_dim,))\n",
    "outputs = layers.Dense(original_dim)(x) # No activation\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.00001, clipnorm=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAE model, with connected layers. We also save the model for future work ect.\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, original_dim=ori_dim, epochs = epochz, intermediate_dim = intermediate_dim, latent_dim = latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        inputs = keras.Input(shape=(original_dim,))\n",
    "        h = layers.Dense(intermediate_dim)(inputs) # delete act\n",
    "        \n",
    "        z_mean = layers.Dense(latent_dim)(h)\n",
    "        z_log_sigma = layers.Dense(latent_dim)(h)\n",
    "        z = layers.Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "        # Create encoder\n",
    "        encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
    "        self.encoder = encoder\n",
    "        # Create decoder\n",
    "        latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "        x = layers.Dense(intermediate_dim)(latent_inputs) #relu\n",
    "        \n",
    "        \n",
    "        outputs = layers.Dense(original_dim)(x) #delete act\n",
    "        decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        # instantiate VAE model\n",
    "        outputs = decoder(encoder(inputs)[2])\n",
    "        vae = keras.Model(inputs, outputs, name='vae_mlp')\n",
    "        \n",
    "        # loss\n",
    "        reconstruction_loss = keras.losses.mean_squared_error(inputs, outputs) #mean and a varriance\n",
    "        reconstruction_loss *= original_dim        \n",
    "        kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5   \n",
    "        vae_loss = K.mean(0.9*(reconstruction_loss) + 0.1*(kl_loss))\n",
    "        vae.add_loss(vae_loss)\n",
    "        \n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.0001 ,clipnorm=0.001)\n",
    "\n",
    "        vae.compile(optimizer= opt, loss='mean_squared_logarithmic_error', metrics=['accuracy'])\n",
    "        history = vae.fit(y_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(y_test, y_test))\n",
    "        vae.save(\"test_vae2\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model on the Gene_expression dataset\n",
    "x_train = x_test = np.array(x) \n",
    "vae = VAE(intermediate_dim = intermediate_dim, latent_dim = latent_dim, epochs=epochz)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plotting and predictions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)\n",
    "# encode\n",
    "x_test_encoded = np.array(vae.encoder.predict(x_test , batch_size=batch_size))\n",
    "#Visualizations , sanity check \n",
    "mapper = umap.UMAP().fit(y)\n",
    "umap.plot.points(mapper)\n",
    "standard_embedding = umap.UMAP(random_state=42).fit_transform(x)\n",
    "plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], s=0.1, cmap='Spectral');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latent space visualization \n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "z_mean = np.array(vae.encoder.predict( outputs , batch_size=120))\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(z_mean[:, 0:500], z_mean[:, 501:1001],c=z_mean[:0:1001])\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "plt.title(\"Latent space\",fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the data\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(df_gene)\n",
    "y_pca = pca.fit_transform(df_isoform)\n",
    "\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "component_names_1 = [f\"PC{i+1}\" for i in range(y_pca.shape[1])]\n",
    "y_pca = pd.DataFrame(y_pca, columns=component_names_1)\n",
    "\n",
    "X = X_pca # training on\n",
    "y = y_pca # target\n",
    "\n",
    "# X_pca.head()\n",
    "principal_df = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2'])\n",
    "\n",
    "# Fit a linear regression model on the transformed data\n",
    "model = LinearRegression().fit(X, y)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Use the model to make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(principal_df.iloc[:,0], principal_df.iloc[:,1], s=40)\n",
    "plt.title('PCA plot in 2D')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "print(X_pca.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used KMeans to cluster together the tissues so we can get labels for our plots , this method it's not the optimal one but we assume tissues are clustering together. \n",
    "\n",
    "# KMeans\n",
    "kmeans = KMeans(n_clusters=24, n_init=15, max_iter=1000, random_state=69)\n",
    "# Train and make predictions\n",
    "clusters = kmeans.fit_predict(predictions)\n",
    "# Cluster centers\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "um = umap.UMAP()\n",
    "X_fit = um.fit(predictions)           \n",
    "X_umap = um.transform(predictions)\n",
    "\n",
    "# Convert to data frame\n",
    "umap_df = pd.DataFrame(data = X_umap, columns = ['umap comp. 1', 'umap comp. 2'])\n",
    "\n",
    "# Shape and preview\n",
    "print(umap_df.shape)\n",
    "umap_df.head()\n",
    "\n",
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(umap_df.iloc[:,0], umap_df.iloc[:,1], c=clusters, cmap=\"brg\", s=40)\n",
    "\n",
    "# Centroids\n",
    "centroids_umap = um.transform(centroids)\n",
    "plt.scatter(x=centroids_umap[:,0], y=centroids_umap[:,1], marker=\"x\", s=500, linewidths=3, color=\"black\")\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('UMAP plot in 2D')\n",
    "plt.xlabel('umap component 1')\n",
    "plt.ylabel('umap component 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UMAP to reduce the dimensionality of the data\n",
    "umap_model = UMAP(n_components=2)\n",
    "X_transformed = umap_model.fit_transform(x)\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_transformed,  test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the random forest model\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Initialize a list to store the validation losses for each iteration\n",
    "val_losses = []\n",
    "\n",
    "# Train the model and calculate the validation loss for each iteration\n",
    "for i in range(100):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    val_loss = np.mean((y_pred - y_val)**2)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "# Plot the validation losses\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow \n",
    "distortions = []\n",
    "K = range(15,50)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(X_transformed)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using UMAP to visualize the 24 clusters of different tissues.\n",
    "mapper = umap.UMAP(min_dist=1 , n_neighbors=25).fit(X_transformed)\n",
    "umap.plot.points(mapper, labels= clusters )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
